{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bdc954ed-c584-47c7-9d61-9b3e20c2ee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varun Bhat\\AppData\\Local\\Temp\\ipykernel_7964\\464842245.py:44: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return obs, reward, done, truncated, np.array(grads)\n",
      "C:\\Users\\Varun Bhat\\AppData\\Local\\Temp\\ipykernel_7964\\464842245.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return obs, reward, done, truncated, np.array(grads)\n",
      "C:\\Users\\Varun Bhat\\AppData\\Local\\Temp\\ipykernel_7964\\464842245.py:61: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return all_rewards, np.array(all_gradients)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "env = gym.make(\"CartPole-v1\",render_mode=\"human\")\n",
    "states_dim = env.observation_space.shape[0]\n",
    "actions_dim = env.action_space.n\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, states_dim=states_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(states_dim, 5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sig1(x)\n",
    "        return x\n",
    "\n",
    "def discount(rewards, gamma):\n",
    "    discounted = rewards\n",
    "    for i in range(discounted.shape[0]-2, 0, -1):\n",
    "        discounted[i] = discounted[i] + gamma*discounted[i+1]\n",
    "    return discounted\n",
    "\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    left = model(torch.tensor(obs))\n",
    "    action = np.array([int(np.random.uniform(0,1,(1,1)) > left.detach().numpy())])\n",
    "    y_target = torch.tensor((np.array([1]) - action), dtype=torch.float32)\n",
    "    loss = loss_fn(left, y_target)\n",
    "    loss.backward()\n",
    "    grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        grads.append(param.grad)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, np.array(grads)\n",
    "\n",
    "def play_multiple_episodes(env, model, loss_fn, episodes, n_max_steps):\n",
    "    all_rewards = []\n",
    "    all_gradients = []\n",
    "    for episode in range(episodes):\n",
    "        current_rewards = []\n",
    "        current_gradients = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_gradients.append(grads)\n",
    "            if done or truncated:\n",
    "                break        \n",
    "        all_rewards.append(np.array(current_rewards))\n",
    "        all_gradients.append(np.array(current_gradients))\n",
    "    return all_rewards, np.array(all_gradients)\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, gamma):\n",
    "    all_discounted_rewards = [discount(rewards, gamma) for rewards in all_rewards]\n",
    "    \n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards-reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "model = PolicyNetwork()\n",
    "\n",
    "iterations = 150\n",
    "episodes = 10\n",
    "n_max_steps = 500\n",
    "gamma = 0.95\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "for iter in range(iterations):\n",
    "    all_rewards, all_gradients = play_multiple_episodes(env, model, loss_fn, episodes, n_max_steps)\n",
    "    advantage = discount_and_normalize_rewards(all_rewards, gamma)\n",
    "    mean_gradients = sum([all_gradients[episode][step]*advantage[episode][step] for episode in range(episodes) for step in range(len(advantage[episode]))])\n",
    "    with torch.no_grad():\n",
    "        for param, grad in zip(mean_gradients, model.parameters()):\n",
    "            param -= lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41fc5258-06a9-43d7-ba56-3a9f1d04e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0302"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantage.shape => (episodes,steps)\n",
    "all_gradients.shape => (episodes,steps,grad_tensor_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fecd3-aa00-4bf1-a69f-e906bc79de20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
